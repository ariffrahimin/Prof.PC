{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Downloading https://files.pythonhosted.org/packages/dd/8d/4c7676d01e90852254e2275fb4639b747274430f2fa066aa94848d3a6ee4/streamlit-0.73.1-py2.py3-none-any.whl (7.4MB)\n",
      "Collecting validators (from streamlit)\n",
      "  Downloading https://files.pythonhosted.org/packages/db/2f/7fed3ee94ad665ad2c1de87f858f10a7785251ff75b4fd47987888d07ef1/validators-0.18.2-py3-none-any.whl\n",
      "Requirement already satisfied: packaging in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from streamlit) (19.2)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from streamlit) (7.0)\n",
      "Collecting pydeck>=0.1.dev5 (from streamlit)\n",
      "  Downloading https://files.pythonhosted.org/packages/9e/9d/8fbf1f56cc5891e6c3295bf94fc176e9ab0a3ffdd090cc8b354ac2640f9a/pydeck-0.5.0-py2.py3-none-any.whl (4.5MB)\n",
      "Requirement already satisfied: blinker in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from streamlit) (1.4)\n",
      "Requirement already satisfied: protobuf!=3.11,>=3.6.0 in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from streamlit) (3.11.2)\n",
      "Collecting pyarrow; python_version < \"3.9\" (from streamlit)\n",
      "  Downloading https://files.pythonhosted.org/packages/d8/c6/f847631e56f4a8143ccceb0c89f927d4e2f32eea22b300acd21b51f22562/pyarrow-2.0.0-cp37-cp37m-win_amd64.whl (10.7MB)\n",
      "Requirement already satisfied: altair>=3.2.0 in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from streamlit) (4.1.0)\n",
      "Collecting tzlocal (from streamlit)\n",
      "  Downloading https://files.pythonhosted.org/packages/5d/94/d47b0fd5988e6b7059de05720a646a2930920fff247a826f61674d436ba4/tzlocal-2.1-py2.py3-none-any.whl\n",
      "Collecting base58 (from streamlit)\n",
      "  Downloading https://files.pythonhosted.org/packages/3c/03/58572025c77b9e6027155b272a1b96298e711cd4f95c24967f7137ab0c4b/base58-2.0.1-py3-none-any.whl\n",
      "Requirement already satisfied: cachetools>=4.0 in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from streamlit) (4.0.0)\n",
      "Requirement already satisfied: pandas>=0.21.0 in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from streamlit) (0.25.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from streamlit) (1.18.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from streamlit) (6.2.0)\n",
      "Collecting toml (from streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/44/6f/7120676b6d73228c96e17f1f794d8ab046fc910d781c8d151120c3f1569e/toml-0.10.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: astor in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from streamlit) (0.8.1)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from streamlit) (2.8.0)\n",
      "Collecting watchdog (from streamlit)\n",
      "  Downloading https://files.pythonhosted.org/packages/b5/db/8b7c15e98bf54931a980e5430838fb35f3a5b112a0f4fb9609a07e80f949/watchdog-1.0.2-py3-none-win_amd64.whl (72kB)\n",
      "Requirement already satisfied: tornado>=5.0 in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from streamlit) (6.0.3)\n",
      "Collecting gitpython (from streamlit)\n",
      "  Downloading https://files.pythonhosted.org/packages/24/d1/a7f8fe3df258549b303415157328bfcc63e9b11d06a7ad7a3327f3d32606/GitPython-3.1.11-py3-none-any.whl (159kB)\n",
      "Requirement already satisfied: requests in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from streamlit) (2.11.1)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from validators->streamlit) (1.12.0)\n",
      "Requirement already satisfied: decorator>=3.4.0 in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from validators->streamlit) (4.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from packaging->streamlit) (2.4.2)\n",
      "Requirement already satisfied: jinja2>=2.10.1 in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from pydeck>=0.1.dev5->streamlit) (2.10.3)\n",
      "Requirement already satisfied: ipywidgets>=7.0.0 in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from pydeck>=0.1.dev5->streamlit) (7.5.1)\n",
      "Requirement already satisfied: ipykernel>=5.1.2; python_version >= \"3.4\" in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from pydeck>=0.1.dev5->streamlit) (5.1.2)\n",
      "Requirement already satisfied: traitlets>=4.3.2 in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from pydeck>=0.1.dev5->streamlit) (4.3.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from protobuf!=3.11,>=3.6.0->streamlit) (41.4.0)\n",
      "Requirement already satisfied: jsonschema in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from altair>=3.2.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from altair>=3.2.0->streamlit) (0.3)\n",
      "Requirement already satisfied: toolz in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from altair>=3.2.0->streamlit) (0.10.0)\n",
      "Requirement already satisfied: pytz in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from tzlocal->streamlit) (2019.3)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython->streamlit)\n",
      "  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from jinja2>=2.10.1->pydeck>=0.1.dev5->streamlit) (1.1.1)\n",
      "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (7.8.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.5.1)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (4.4.0)\n",
      "Requirement already satisfied: jupyter-client in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (5.3.3)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from traitlets>=4.3.2->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from jsonschema->altair>=3.2.0->streamlit) (0.15.4)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from jsonschema->altair>=3.2.0->streamlit) (19.2.0)\n",
      "Collecting smmap<4,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython->streamlit)\n",
      "  Downloading https://files.pythonhosted.org/packages/b0/9a/4d409a6234eb940e6a78dfdfc66156e7522262f5f2fecca07dc55915952d/smmap-3.0.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (2.0.10)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.4.1)\n",
      "Requirement already satisfied: jedi>=0.10 in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.15.1)\n",
      "Requirement already satisfied: backcall in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.1.0)\n",
      "Requirement already satisfied: pygments in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (2.4.2)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.5)\n",
      "Requirement already satisfied: notebook>=4.4.1 in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (6.0.1)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (4.5.0)\n",
      "Requirement already satisfied: pywin32>=1.0; sys_platform == \"win32\" in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (223)\n",
      "Requirement already satisfied: pyzmq>=13 in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (18.1.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.1.7)\n",
      "Requirement already satisfied: parso>=0.5.0 in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from jedi>=0.10->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.1)\n",
      "Requirement already satisfied: Send2Trash in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.5.0)\n",
      "Requirement already satisfied: terminado>=0.8.1 in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.2)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.6.0)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.1)\n",
      "Requirement already satisfied: testpath in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.4.2)\n",
      "Requirement already satisfied: bleach in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.1.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.6.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.4.2)\n",
      "Requirement already satisfied: webencodings in c:\\users\\rnd\\anaconda3\\lib\\site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.1)\n",
      "Installing collected packages: validators, pydeck, pyarrow, tzlocal, base58, toml, watchdog, smmap, gitdb, gitpython, streamlit\n",
      "Successfully installed base58-2.0.1 gitdb-4.0.5 gitpython-3.1.11 pyarrow-2.0.0 pydeck-0.5.0 smmap-3.0.4 streamlit-0.73.1 toml-0.10.2 tzlocal-2.1 validators-0.18.2 watchdog-1.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'google.protobuf.descriptor' has no attribute '_internal_create_key'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-e215e83c4c7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mstreamlit\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mst\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlancaster\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLancasterStemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mstemmer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLancasterStemmer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\streamlit\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstreamlit\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogger\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_logger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstreamlit\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfig\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mstreamlit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRootContainer_pb2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRootContainer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[0m_LOGGER\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_logger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_logger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"root\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\streamlit\\proto\\RootContainer_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m   \u001b[0msyntax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'proto3'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m   \u001b[0mserialized_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m   \u001b[0mcreate_key\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_descriptor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_internal_create_key\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m   \u001b[0mserialized_pb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mb'\\n#streamlit/proto/RootContainer.proto*&\\n\\rRootContainer\\x12\\x08\\n\\x04MAIN\\x10\\x00\\x12\\x0b\\n\\x07SIDEBAR\\x10\\x01\\x62\\x06proto3'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'google.protobuf.descriptor' has no attribute '_internal_create_key'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import streamlit as st\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "import numpy\n",
    "import tflearn\n",
    "import tensorflow\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "with open(\"dataset/games.json\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "try:\n",
    "    with open(\"data.pickle\", \"rb\") as f:\n",
    "        words, labels, training, output = pickle.load(f)\n",
    "except:\n",
    "    words = []\n",
    "    labels = []\n",
    "    docs_x = []\n",
    "    docs_y = []\n",
    "\n",
    "    for intent in data[\"intents\"]:\n",
    "        for pattern in intent[\"patterns\"]:\n",
    "            wrds = nltk.word_tokenize(pattern)\n",
    "            words.extend(wrds)\n",
    "            docs_x.append(wrds)\n",
    "            docs_y.append(intent[\"tag\"])\n",
    "\n",
    "        if intent[\"tag\"] not in labels:\n",
    "            labels.append(intent[\"tag\"])\n",
    "\n",
    "    words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n",
    "    words = sorted(list(set(words)))\n",
    "\n",
    "    labels = sorted(labels)\n",
    "\n",
    "    training = []\n",
    "    output = []\n",
    "\n",
    "    out_empty = [0 for _ in range(len(labels))]\n",
    "\n",
    "    for x, doc in enumerate(docs_x):\n",
    "        bag = []\n",
    "\n",
    "        wrds = [stemmer.stem(w.lower()) for w in doc]\n",
    "\n",
    "        for w in words:\n",
    "            if w in wrds:\n",
    "                bag.append(1)\n",
    "            else:\n",
    "                bag.append(0)\n",
    "\n",
    "        output_row = out_empty[:]\n",
    "        output_row[labels.index(docs_y[x])] = 1\n",
    "\n",
    "        training.append(bag)\n",
    "        output.append(output_row)\n",
    "\n",
    "\n",
    "    training = numpy.array(training)\n",
    "    output = numpy.array(output)\n",
    "\n",
    "    with open(\"data.pickle\", \"wb\") as f:\n",
    "        pickle.dump((words, labels, training, output), f)\n",
    "\n",
    "tensorflow.reset_default_graph()\n",
    "\n",
    "net = tflearn.input_data(shape=[None, len(training[0])])\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\")\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "model = tflearn.DNN(net)\n",
    "\n",
    "# try:\n",
    "model.load(\"model_2.tflearn\")\n",
    "# except:\n",
    "# model.fit(training, output, n_epoch=500, batch_size=8, show_metric=True)\n",
    "# model.save(\"model_2.tflearn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start talking with the bot (type quit to stop)!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'st' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-41aa72183f4d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;31m#         print(random.choice(responses))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[0mchat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-41aa72183f4d>\u001b[0m in \u001b[0;36mchat\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m#         inp = input(\"You: \")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0minp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"quit\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-41aa72183f4d>\u001b[0m in \u001b[0;36mget_text\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0minput_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"You: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"type here\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mdf_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput_text\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'questions'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf_input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'st' is not defined"
     ]
    }
   ],
   "source": [
    "def bag_of_words(s, words):\n",
    "    bag = [0 for _ in range(len(words))]\n",
    "\n",
    "    s_words = nltk.word_tokenize(s)\n",
    "    s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
    "\n",
    "    for se in s_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == se:\n",
    "                bag[i] = 1\n",
    "            \n",
    "    return numpy.array(bag)\n",
    "\n",
    "\n",
    "def get_text():\n",
    "    input_text = st.text_input(\"You: \",\"type here\")\n",
    "    df_input = pd.DataFrame([input_text],columns=['questions'])\n",
    "    return df_input \n",
    "\n",
    "def chat():\n",
    "    print(\"Start talking with the bot (type quit to stop)!\")\n",
    "    while True:\n",
    "#         inp = input(\"You: \")\n",
    "        inp = get_text()\n",
    "        if inp.lower() == \"quit\":\n",
    "            break\n",
    "\n",
    "        results = model.predict([bag_of_words(inp, words)])\n",
    "        results_index = numpy.argmax(results)\n",
    "        tag = labels[results_index]\n",
    "\n",
    "        for tg in data[\"intents\"]:\n",
    "            if tg['tag'] == tag:\n",
    "                responses = tg['responses']\n",
    "                \n",
    "        st.text_area(\"Bot:\", value=response, height=200, max_chars=None, key=None)\n",
    "#         print(random.choice(responses))\n",
    "\n",
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import string\n",
    "import re\n",
    "import joblib\n",
    "import json\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, Flatten\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(jsonFile):\n",
    "    with open(jsonFile) as file:\n",
    "        Json_data = json.loads(file.read())\n",
    "    return Json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_doc('dataset/intents.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_data(feat_1,feat_2,is_pattern):\n",
    "    is_pattern = is_pattern\n",
    "    df = pd.DataFrame(columns=[feat_1,feat_2])\n",
    "    for intent in data['intents']:\n",
    "        if is_pattern:\n",
    "            for pattern in intent['patterns']:\n",
    "                w = pattern\n",
    "                df_to_append = pd.Series([w,intent['tag']], index = df.columns)\n",
    "                df = df.append(df_to_append,ignore_index=True)\n",
    "        else:\n",
    "            for response in intent['responses']:\n",
    "                w = response\n",
    "                df_to_append = pd.Series([w,intent['tag']], index = df.columns)\n",
    "                df = df.append(df_to_append,ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# users intents \n",
    "df1 = frame_data('questions','labels',True)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.labels.value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bot response\n",
    "df2 = frame_data('response','labels',False)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "vocab = Counter()\n",
    "labels = []\n",
    "def tokenizer(entry):\n",
    "    tokens = entry.split()\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    tokens = [lemmatizer.lemmatize(w.lower()) for w in tokens]\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     tokens = [w for w in tokens if not w in stop_words]\n",
    "    tokens = [word.lower() for word in tokens if len(word) > 1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(tokenizer,df,feature):\n",
    "    doc_without_stopwords = []\n",
    "    for entry in df[feature]:\n",
    "        tokens = tokenizer(entry)\n",
    "        joblib.dump(tokens,'tokens.pkl')\n",
    "        doc_without_stopwords.append(' '.join(tokens))\n",
    "    df[feature] = doc_without_stopwords\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(tokenizer,df,feature):\n",
    "    for entry in df[feature]:\n",
    "        tokens = tokenizer(entry)   \n",
    "        vocab.update(tokens)\n",
    "    joblib.dump(vocab,'vocab.pkl')\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_vocab(tokenizer,df1,'questions')\n",
    "remove_stop_words(tokenizer,df1,'questions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_list = list(df1.groupby(by='labels',as_index=False).first()['questions'])\n",
    "test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_index = []\n",
    "for i,_ in enumerate(test_list):\n",
    "    idx = df1[df1.questions == test_list[i]].index[0]\n",
    "    test_index.append(idx)\n",
    "len(test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = [i for i in df1.index if i not in test_index]\n",
    "len(train_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(list(vocab.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(df,feature):\n",
    "#     text = ' '.join(list(vocab.keys()))\n",
    "    t = Tokenizer()\n",
    "    entries = [entry for entry in df[feature]]\n",
    "    t.fit_on_texts(entries)\n",
    "    joblib.dump(t,'tokenizer_t.pkl')\n",
    "    vocab_size = len(t.word_index) + 1\n",
    "    entries = [entry for entry in df[feature]]\n",
    "    max_length = max([len(s.split()) for s in entries])\n",
    "    encoded = t.texts_to_sequences(entries)\n",
    "    padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
    "    \n",
    "    return padded, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,vocab_size = encoder(df1,'questions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_encoded['labels'] = df1.labels\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,2):\n",
    "    dt = [0]*8\n",
    "    dt.append('confused')\n",
    "    dt = [dt]\n",
    "    pd.DataFrame(dt).rename(columns = {8:'labels'})\n",
    "    df_encoded = df_encoded.append(pd.DataFrame(dt).rename(columns = {8:'labels'}),ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_encoded = df_encoded.append(pd.DataFrame(dt).rename(columns = {16:'labels'}),ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "lable_enc = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labl = lable_enc.fit_transform(df_encoded.labels)\n",
    "labl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mapper = {}\n",
    "for index,key in enumerate(df_encoded.labels):\n",
    "    if key not in mapper.keys():\n",
    "        mapper[key] = labl[index]\n",
    "mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.labels = df2.labels.map(mapper).astype({'labels': 'int32'})\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('response.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.groupby('labels').get_group(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_encoded.loc[train_index]\n",
    "test = df_encoded.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(columns=['labels'],axis=1)\n",
    "y_train = train.labels\n",
    "X_test = test.drop(columns=['labels'],axis=1)\n",
    "y_test = test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train =pd.get_dummies(y_train).values\n",
    "y_test =pd.get_dummies(y_test).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = X_train.shape[1]\n",
    "# output = len(df3.labels.unique())\n",
    "output = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def define_model(vocab_size, max_length):\n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(vocab_size,300, input_length=max_length))\n",
    "#     model.add(Conv1D(filters=64, kernel_size=6, activation='relu'))\n",
    "#     model.add(MaxPooling1D(pool_size=4))\n",
    "#     model.add(Flatten())\n",
    "# #     model.add(Dense(32, activation='relu'))\n",
    "#     model.add(Dense(16, activation='softmax'))\n",
    "    \n",
    "#     # compile network\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#     # summarize defined model\n",
    "#     model.summary()\n",
    "# #     plot_model(model, to_file='model.png', show_shapes=True)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss',patience=10)\n",
    "checkpoint = ModelCheckpoint(\"model-v1.h5\",\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 3, verbose = 1, min_delta = 0.0001)\n",
    "callbacks = [early_stopping,checkpoint,reduce_lr]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def define_model(vocab_size, max_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size,300, input_length=max_length))\n",
    "    model.add(Conv1D(filters=64, kernel_size=4, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=5))\n",
    "    model.add(Flatten())\n",
    "#     model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(8, activation='softmax'))\n",
    "    \n",
    "    \n",
    "    # compile network\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.compile(loss = 'categorical_crossentropy',\n",
    "              # optimizer = Adam(lr=0.001),\n",
    "              optimizer = 'adam',\n",
    "              metrics = ['accuracy'])\n",
    "    \n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "#     plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "model = define_model(vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=500, verbose=1, validation_data=(X_test,y_test), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.argmax(i) for i in model.predict(X_test)][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.argmax(i) for i in y_test][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New user question encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text():\n",
    "    input_text  = ['what are you']\n",
    "    df_input = pd.DataFrame(input_text,columns=['questions'])\n",
    "    df_input\n",
    "    return df_input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load artifacts \n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('model-v1.h5')\n",
    "tokenizer_t = joblib.load('tokenizer_t.pkl')\n",
    "vocab = joblib.load('vocab.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(entry):\n",
    "    tokens = entry.split()\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    tokens = [lemmatizer.lemmatize(w.lower()) for w in tokens]\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     tokens = [w for w in tokens if not w in stop_words]\n",
    "    tokens = [word.lower() for word in tokens if len(word) > 1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words_for_input(tokenizer,df,feature):\n",
    "    doc_without_stopwords = []\n",
    "    entry = df[feature][0]\n",
    "    tokens = tokenizer(entry)\n",
    "    doc_without_stopwords.append(' '.join(tokens))\n",
    "    df[feature] = doc_without_stopwords\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_input = remove_stop_words_for_input(tokenizer,df_input,'questions')\n",
    "# df_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_input_text(tokenizer_t,df,feature):\n",
    "    t = tokenizer_t\n",
    "    entry = entry = [df[feature][0]]\n",
    "    encoded = t.texts_to_sequences(entry)\n",
    "    padded = pad_sequences(encoded, maxlen=16, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_input = encode_input_text(tokenizer_t,df_input,'questions')\n",
    "# encoded_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(model,encoded_input):\n",
    "    pred = np.argmax(model.predict(encoded_input))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bot_precausion(df_input,pred):\n",
    "    words = df_input.questions[0].split()\n",
    "    if len([w for w in words if w in vocab])==0 :\n",
    "        pred = 1\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(df2,pred):\n",
    "    upper_bound = df2.groupby('labels').get_group(pred).shape[0]\n",
    "    r = np.random.randint(0,upper_bound)\n",
    "    responses = list(df2.groupby('labels').get_group(pred).response)\n",
    "    return responses[r]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bot_response(response,):\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = get_text()\n",
    "\n",
    "#load artifacts \n",
    "tokenizer_t = joblib.load('tokenizer_t.pkl')\n",
    "vocab = joblib.load('vocab.pkl')\n",
    "\n",
    "df_input = remove_stop_words_for_input(tokenizer,df_input,'questions')\n",
    "encoded_input = encode_input_text(tokenizer_t,df_input,'questions')\n",
    "\n",
    "pred = get_pred(model,encoded_input)\n",
    "pred = bot_precausion(df_input,pred)\n",
    "\n",
    "response = get_response(df2,pred)\n",
    "bot_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.image()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
